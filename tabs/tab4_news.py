# -*- coding: utf-8 -*-
"""
Tab 4: News - ê¸€ë¡œë²Œ & êµ­ë‚´ ì»¤í”¼ ë‰´ìŠ¤ ì¸ì‚¬ì´íŠ¸
"""

import streamlit as st
import feedparser
import os
from dotenv import load_dotenv
import requests
import re

load_dotenv()

# ë„¤ì´ë²„ API í‚¤
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "")

# ==========================================
# ë²ˆì—­ ë° ë¶„ì„ í•¨ìˆ˜
# ==========================================
def get_translator():
    try:
        from deep_translator import GoogleTranslator
        return GoogleTranslator(source='auto', target='ko')
    except:
        return None

def translate_text(text):
    try:
        if not text:
            return ""
        translator = get_translator()
        if translator:
            return translator.translate(text[:4999])
        return text
    except:
        return text

def get_article_summary(url):
    try:
        from newspaper import Article, Config
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        config = Config()
        config.browser_user_agent = user_agent
        config.request_timeout = 10
        article = Article(url, config=config)
        article.download()
        article.parse()
        article.nlp()
        summary = article.summary
        if not summary:
            return "âš ï¸ ìš”ì•½ ì‹¤íŒ¨ (ë³¸ë¬¸ ì¶”ì¶œ ë¶ˆê°€)"
        return summary 
    except Exception as e:
        return f"ğŸš« ì—ëŸ¬ ë°œìƒ: {str(e)}"

def analyze_sentiment(text):
    try:
        from textblob import TextBlob
        blob = TextBlob(text)
        score = blob.sentiment.polarity
        if score > 0.1:
            return "ğŸŸ¢ ê¸ì •ì "
        elif score < -0.1:
            return "ğŸ”´ ë¶€ì •ì "
        else:
            return "âšª ì¤‘ë¦½ì "
    except:
        return "âšª ë¶„ì„ë¶ˆê°€"

def display_wordcloud(news_list):
    if not news_list:
        return
    try:
        from wordcloud import WordCloud
        import matplotlib.pyplot as plt
        
        text = " ".join([item.get('ì›ì œ', '') for item in news_list if item.get('ì›ì œ')])
        if not text:
            return
        wc = WordCloud(width=800, height=400, background_color='white', colormap='copper', max_words=80).generate(text)
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
    except Exception as e:
        st.warning(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {e}")

# ==========================================
# ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜
# ==========================================
def fetch_google_news(query, target_keywords=None, period='30d'):
    noise_filter = "-Starbucks -store -closing -opened -travel -vacation -hotel -resort -tourism"
    full_query = f"{query} {noise_filter}"
    encoded_query = full_query.replace(" ", "%20")
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}+when:{period}&hl=en-US&gl=US&ceid=US:en"
    
    feed = feedparser.parse(rss_url)
    news_list = []
    seen_titles = set()
    coffee_guard_terms = ["coffee", "bean", "arabica", "robusta", "commodity", "harvest", "crop", "farm", "roast", "export", "production"]

    if not feed.entries:
        return []

    count = 0
    for entry in feed.entries[:100]:
        if count >= 50:
            break
        
        title_en = entry.title
        link = entry.link
        summary_text = entry.get('summary', '') 
        title_signature = title_en[:30].lower()
        if title_signature in seen_titles:
            continue
        
        content_to_check = (title_en + " " + summary_text).lower()
        
        if target_keywords:
            has_target = any(k.lower() in content_to_check for k in target_keywords)
            has_coffee_context = any(term in content_to_check for term in coffee_guard_terms)
            if not (has_target and has_coffee_context):
                continue 

        seen_titles.add(title_signature)
        sentiment = analyze_sentiment(title_en)
        korean_title = translate_text(title_en)
        
        news_list.append({
            "ì œëª©": korean_title,
            "ì›ì œ": title_en,
            "ë§í¬": link,
            "ê²Œì‹œì¼": entry.published,
            "ê°ì„±": sentiment
        })
        count += 1
    return news_list

def fetch_naver_news_api(query):
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        return [{"ì œëª©": "âš ï¸ API í‚¤ ë¯¸ì„¤ì •: .env íŒŒì¼ì— NAVER_CLIENT_ID, NAVER_CLIENT_SECRETì„ ì„¤ì •í•´ì£¼ì„¸ìš”.", "ë§í¬": "#", "ê²Œì‹œì¼": "", "ì–¸ë¡ ì‚¬": "ì‹œìŠ¤í…œ"}]

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {"query": query, "display": 10, "sort": "sim"}

    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        if response.status_code == 200:
            items = response.json().get('items', [])
            results = []
            for item in items:
                clean_title = re.sub('<.*?>', '', item['title']).replace("&quot;", "'").replace("&amp;", "&")
                link = item['originallink'] if item['originallink'] else item['link']
                pub_date = item['pubDate'][:16]
                
                results.append({
                    "ì œëª©": clean_title,
                    "ë§í¬": link,
                    "ê²Œì‹œì¼": pub_date,
                    "ì–¸ë¡ ì‚¬": "ë„¤ì´ë²„ë‰´ìŠ¤"
                })
            return results
        else:
            return [{"ì œëª©": f"âš ï¸ í†µì‹  ì˜¤ë¥˜ (Code: {response.status_code})", "ë§í¬": "#", "ê²Œì‹œì¼": "", "ì–¸ë¡ ì‚¬": "ì˜¤ë¥˜"}]
    except Exception as e:
        return [{"ì œëª©": f"âš ï¸ ì—ëŸ¬: {str(e)}", "ë§í¬": "#", "ê²Œì‹œì¼": "", "ì–¸ë¡ ì‚¬": "ì˜¤ë¥˜"}]

# ==========================================
# ë©”ì¸ show í•¨ìˆ˜
# ==========================================
def show():
    """News í˜ì´ì§€ë¥¼ ë Œë”ë§í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    
    st.markdown("""
        <style>
        .stApp { background-color: #FDFbf7; }
        h1, h2, h3, p, span, div, label { color: #000000 !important; }
        div.stButton > button { background-color: #6F4E37; color: #FFFFFF !important; border-radius: 5px; }
        </style>
    """, unsafe_allow_html=True)

    st.title("â˜• Global & Local ì»¤í”¼ ì¸ì‚¬ì´íŠ¸")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'risk_news' not in st.session_state:
        st.session_state['risk_news'] = []
    if 'origin_news' not in st.session_state:
        st.session_state['origin_news'] = []
    if 'korea_news' not in st.session_state:
        st.session_state['korea_news'] = []

    tab1, tab2, tab3 = st.tabs(["ğŸ”¥ ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬", "ğŸŒ ì‚°ì§€ë³„ ë™í–¥", "ğŸ‡°ğŸ‡· êµ­ë‚´ ì‹œì¥ ë‰´ìŠ¤"])

    # Tab 1: ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬
    with tab1:
        st.subheader("ê¸€ë¡œë²Œ ê³µê¸‰ë§ & ì •ì±… ë¦¬ìŠ¤í¬")
        if st.button("ë¦¬ìŠ¤í¬ ë‰´ìŠ¤ ê²€ìƒ‰ (Google)", key="btn_risk"):
            with st.spinner('í•´ì™¸ ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„ ì¤‘...'):
                q = "Coffee Supply Chain OR EUDR Regulation OR Red Sea Logistics"
                targets = ["Coffee", "EUDR", "Red Sea", "Supply", "Logistics", "Price", "Regulation"]
                st.session_state['risk_news'] = fetch_google_news(q, targets, period='365d')
                
        if st.session_state['risk_news']:
            display_wordcloud(st.session_state['risk_news'])
            st.divider()
            for i, item in enumerate(st.session_state['risk_news'][:10]):
                with st.expander(f"[{item['ê°ì„±']}] {item['ì œëª©']}"):
                    st.caption(item['ê²Œì‹œì¼'])
                    st.write(f"ì›ì œ: {item['ì›ì œ']}")
                    st.markdown(f"[ê¸°ì‚¬ ë³´ê¸°]({item['ë§í¬']})")
                    if st.button("ìš”ì•½ (ì˜ë¬¸ ê¸°ì‚¬)", key=f"risk_{i}"):
                        summary = get_article_summary(item['ë§í¬'])
                        st.success(translate_text(summary))

    # Tab 2: ì‚°ì§€ë³„ ë™í–¥
    with tab2:
        st.subheader("ì£¼ìš” ì‚°ì§€ë³„ ë™í–¥")
        country = st.selectbox("êµ­ê°€ ì„ íƒ", ["Brazil", "Vietnam", "Colombia", "Ethiopia", "Indonesia", "Kenya", "Honduras", "Guatemala", "Costa Rica", "Peru"], key="news_country")
        
        def get_params(c):
            if c == "Vietnam":
                return '"Vietnam Coffee" (Export OR Production OR Price)', ["Vietnam", "Robusta"]
            elif c == "Brazil":
                return '"Brazil Coffee" (Harvest OR Export OR Crop)', ["Brazil", "Arabica"]
            else:
                return f'"{c} Coffee" (Export OR Price)', [c]

        if st.button(f"{country} ë‰´ìŠ¤ ê²€ìƒ‰ (Google)", key="btn_origin"):
            with st.spinner('í•´ì™¸ ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„ ì¤‘...'):
                query, targets = get_params(country)
                period = '365d' if country in ["Guatemala", "Costa Rica", "Peru", "Honduras", "Kenya"] else '90d'
                st.session_state['origin_news'] = fetch_google_news(query, targets, period=period)
                
        if st.session_state['origin_news']:
            display_wordcloud(st.session_state['origin_news'])
            st.divider()
            for i, item in enumerate(st.session_state['origin_news'][:10]):
                with st.expander(f"[{item['ê°ì„±']}] {item['ì œëª©']}"):
                    st.caption(item['ê²Œì‹œì¼'])
                    st.write(f"ì›ì œ: {item['ì›ì œ']}")
                    st.markdown(f"[ê¸°ì‚¬ ë³´ê¸°]({item['ë§í¬']})")
                    if st.button("ìš”ì•½ (ì˜ë¬¸ ê¸°ì‚¬)", key=f"origin_{i}"):
                        summary = get_article_summary(item['ë§í¬'])
                        st.success(translate_text(summary))

    # Tab 3: êµ­ë‚´ ë‰´ìŠ¤
    with tab3:
        st.subheader("ğŸ‡°ğŸ‡· êµ­ë‚´ ì»¤í”¼ ì‹œì¥ & ì›ë‘ ë‰´ìŠ¤")
        
        if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
            st.warning("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— NAVER_CLIENT_ID, NAVER_CLIENT_SECRETì„ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        
        korea_keyword = st.radio("ê´€ì‹¬ í‚¤ì›Œë“œ ì„ íƒ", ["ì»¤í”¼ ì›ë‘ ê°€ê²©", "ìƒë‘ ìˆ˜ì…", "ì¹´í˜ ì°½ì—… ì‹œì¥", "ìŠ¤í˜ì…œí‹° ì»¤í”¼", "ì €ê°€ ì»¤í”¼ í”„ëœì°¨ì´ì¦ˆ"], horizontal=True, key="korea_kw")
        
        if st.button("êµ­ë‚´ ë‰´ìŠ¤ ê²€ìƒ‰ (Naver API)", key="btn_korea"):
            with st.spinner(f"ë„¤ì´ë²„ì—ì„œ '{korea_keyword}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤..."):
                st.session_state['korea_news'] = fetch_naver_news_api(korea_keyword)
                
        if st.session_state['korea_news']:
            st.success(f"ê²€ìƒ‰ ê²°ê³¼ {len(st.session_state['korea_news'])}ê±´")
            for i, item in enumerate(st.session_state['korea_news']):
                with st.container():
                    st.markdown(f"**{i+1}. {item['ì œëª©']}**")
                    st.caption(f"ğŸ“… {item['ê²Œì‹œì¼']}")
                    st.markdown(f"[ê¸°ì‚¬ ì›ë¬¸ ì½ê¸°]({item['ë§í¬']})")
                    st.divider()
