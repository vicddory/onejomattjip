import streamlit as st
import feedparser
import pandas as pd
from deep_translator import GoogleTranslator
from newspaper import Article, Config
import nltk
from textblob import TextBlob
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import requests
import re 

# ==========================================
# ğŸ”‘ [í•„ìˆ˜] ë„¤ì´ë²„ API í‚¤ ì…ë ¥
# ==========================================
# ë„¤ì´ë²„ ê°œë°œì ì„¼í„°ì—ì„œ ë°œê¸‰ë°›ì€ í‚¤ë¥¼ ì—¬ê¸°ì— ë„£ìœ¼ì„¸ìš”.
NAVER_CLIENT_ID = "ë„¤ì´ë²„ API ID"
NAVER_CLIENT_SECRET = "ë„¤ì´ë²„ API ë¹„ë°€ë²ˆí˜¸"

# --- [ì´ˆê¸° ì„¤ì •] ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# --- [ìŠ¤íƒ€ì¼ ì ìš©] ---
st.markdown(
    """
    <style>
    .stApp { background-color: #FDFbf7; }
    h1, h2, h3, p, span, div, label, .stMarkdown, .stTab { color: #000000 !important; }
    button[data-baseweb="tab"] p { color: #000000 !important; font-weight: bold; }
    [data-testid="stSidebar"] { background-color: #F5F0E6; }
    div.stButton > button { background-color: #6F4E37; color: #FFFFFF !important; border-radius: 5px; }
    </style>
    """,
    unsafe_allow_html=True
)

# --- [ê³µí†µ í•¨ìˆ˜] ë²ˆì—­ ë° ìš”ì•½ ---
@st.cache_resource
def get_translator():
    return GoogleTranslator(source='auto', target='ko')

def translate_text(text):
    try:
        if not text: return ""
        translator = get_translator()
        return translator.translate(text[:4999])
    except:
        return text

def get_article_summary(url):
    try:
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config = Config()
        config.browser_user_agent = user_agent
        config.request_timeout = 10
        article = Article(url, config=config)
        article.download()
        article.parse()
        article.nlp()
        summary = article.summary
        if not summary: return "âš ï¸ ìš”ì•½ ì‹¤íŒ¨ (ë³¸ë¬¸ ì¶”ì¶œ ë¶ˆê°€)"
        return summary 
    except Exception as e:
        return f"ğŸš« ì—ëŸ¬ ë°œìƒ: {str(e)}"

def analyze_sentiment(text):
    blob = TextBlob(text)
    score = blob.sentiment.polarity
    if score > 0.1: return "ğŸŸ¢ ê¸ì •ì "
    elif score < -0.1: return "ğŸ”´ ë¶€ì •ì "
    else: return "âšª ì¤‘ë¦½ì "

def display_wordcloud(news_list):
    if not news_list: return
    text = " ".join([item['ì›ì œ'] for item in news_list])
    wc = WordCloud(width=800, height=400, background_color='white', colormap='copper', max_words=80).generate(text)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off')
    st.pyplot(fig)

# --- [í•µì‹¬ í•¨ìˆ˜ 1] êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘ (í•´ì™¸ìš© - RSS) ---
def fetch_google_news(query, target_keywords=None, period='30d'):
    # ë…¸ì´ì¦ˆ í•„í„°
    noise_filter = "-Starbucks -store -closing -opened -travel -vacation -hotel -resort -tourism -trip -guide -rice -voting -election -visa -immigration"
    full_query = f"{query} {noise_filter}"
    encoded_query = full_query.replace(" ", "%20")
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}+when:{period}&hl=en-US&gl=US&ceid=US:en"
    
    feed = feedparser.parse(rss_url)
    news_list = []
    seen_titles = set()
    # ì»¤í”¼ ë¬¸ë§¥ í•„í„°
    coffee_guard_terms = ["coffee", "bean", "arabica", "robusta", "commodity", "harvest", "crop", "farm", "roast", "export", "production"]

    if not feed.entries: return []

    count = 0
    # 1ë…„ì¹˜ ë°ì´í„°ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ 100ê°œê¹Œì§€ íƒìƒ‰
    for entry in feed.entries[:100]:
        if count >= 50: break # ë¶„ì„ìš© ìµœëŒ€ 50ê°œ
        
        title_en = entry.title
        link = entry.link
        summary_text = entry.get('summary', '') 
        title_signature = title_en[:30].lower()
        if title_signature in seen_titles: continue
        
        content_to_check = (title_en + " " + summary_text).lower()
        
        if target_keywords:
            has_target = any(k.lower() in content_to_check for k in target_keywords)
            has_coffee_context = any(term in content_to_check for term in coffee_guard_terms)
            if not (has_target and has_coffee_context): continue 

        seen_titles.add(title_signature)
        sentiment = analyze_sentiment(title_en)
        korean_title = translate_text(title_en)
        
        news_list.append({
            "ì œëª©": korean_title,
            "ì›ì œ": title_en,
            "ë§í¬": link,
            "ê²Œì‹œì¼": entry.published,
            "ê°ì„±": sentiment
        })
        count += 1
    return news_list

# --- [í•µì‹¬ í•¨ìˆ˜ 2] ë„¤ì´ë²„ ë‰´ìŠ¤ API (êµ­ë‚´ìš© - OpenAPI) ---
def fetch_naver_news_api(query):
    if "ë³¸ì¸ì˜" in NAVER_CLIENT_ID:
        return [{"ì œëª©": "âš ï¸ API í‚¤ ë¯¸ì„¤ì •: ì½”ë“œ ìƒë‹¨ì— í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "ë§í¬": "#", "ê²Œì‹œì¼": "", "ì–¸ë¡ ì‚¬": "ì‹œìŠ¤í…œ"}]

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {
        "query": query,
        "display": 10, # 10ê°œ ì¶œë ¥
        "sort": "sim"  # ê´€ë ¨ë„ìˆœ
    }

    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            items = response.json().get('items', [])
            results = []
            for item in items:
                # HTML íƒœê·¸ ì œê±° ë° íŠ¹ìˆ˜ë¬¸ì ë³€í™˜
                clean_title = re.sub('<.*?>', '', item['title']).replace("&quot;", "'").replace("&amp;", "&")
                link = item['originallink'] if item['originallink'] else item['link']
                # ë‚ ì§œ í¬ë§· ì •ë¦¬ (ì˜ˆ: Mon, 02 Feb 2026...)
                pub_date = item['pubDate'][:16]
                
                results.append({
                    "ì œëª©": clean_title,
                    "ë§í¬": link,
                    "ê²Œì‹œì¼": pub_date,
                    "ì–¸ë¡ ì‚¬": "ë„¤ì´ë²„ë‰´ìŠ¤" # APIëŠ” ì–¸ë¡ ì‚¬ëª…ì„ ì§ì ‘ ì•ˆì¤˜ì„œ í†µì¼
                })
            return results
        else:
            return [{"ì œëª©": f"âš ï¸ í†µì‹  ì˜¤ë¥˜ (Code: {response.status_code})", "ë§í¬": "#", "ê²Œì‹œì¼": "", "ì–¸ë¡ ì‚¬": "ì˜¤ë¥˜"}]
    except Exception as e:
        return [{"ì œëª©": f"âš ï¸ ì—ëŸ¬: {str(e)}", "ë§í¬": "#", "ê²Œì‹œì¼": "", "ì–¸ë¡ ì‚¬": "ì˜¤ë¥˜"}]

# ==========================================
# ğŸš€ ë©”ì¸ ë¡œì§ (UI)
# ==========================================

st.title("â˜• Global & Local ì»¤í”¼ ì¸ì‚¬ì´íŠ¸")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'risk_news' not in st.session_state: st.session_state['risk_news'] = []
if 'origin_news' not in st.session_state: st.session_state['origin_news'] = []
if 'korea_news' not in st.session_state: st.session_state['korea_news'] = []
if 'summary_cache' not in st.session_state: st.session_state['summary_cache'] = {}

tab1, tab2, tab3 = st.tabs(["ğŸ”¥ ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬", "ğŸŒ ì‚°ì§€ë³„ ë™í–¥", "ğŸ‡°ğŸ‡· êµ­ë‚´ ì‹œì¥ ë‰´ìŠ¤"])

# --- [Tab 1] ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬ (Google) ---
with tab1:
    st.subheader("ê¸€ë¡œë²Œ ê³µê¸‰ë§ & ì •ì±… ë¦¬ìŠ¤í¬")
    
    # ğŸ”¥ [ìˆ˜ì •] ë°ì´í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´(ì²« ì‹¤í–‰ ì‹œ) ìë™ìœ¼ë¡œ ê²€ìƒ‰ ì‹¤í–‰
    if not st.session_state['risk_news']:
        with st.spinner('ìµœì‹  ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬ ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
            q = "Coffee Supply Chain OR EUDR Regulation OR Red Sea Logistics"
            targets = ["Coffee", "EUDR", "Red Sea", "Supply", "Logistics", "Price", "Regulation"]
            st.session_state['risk_news'] = fetch_google_news(q, targets, period='365d')

    # ğŸ”¥ [ìˆ˜ì •] ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ (ì´ë¯¸ ë°ì´í„°ê°€ ìˆì–´ë„ ê°•ì œë¡œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜´)
    if st.button("ğŸ”„ ë‰´ìŠ¤ ìƒˆë¡œê³ ì¹¨", key="btn_risk_refresh"):
        with st.spinner('ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¶„ì„ ì¤‘...'):
            q = "Coffee Supply Chain OR EUDR Regulation OR Red Sea Logistics"
            targets = ["Coffee", "EUDR", "Red Sea", "Supply", "Logistics", "Price", "Regulation"]
            st.session_state['risk_news'] = fetch_google_news(q, targets, period='365d')
            st.rerun() # í™”ë©´ ì¦‰ì‹œ ê°±ì‹ 

    # ë‰´ìŠ¤ ì¶œë ¥ ë¡œì§
    if st.session_state['risk_news']:
        display_wordcloud(st.session_state['risk_news'])
        st.divider()
        for i, item in enumerate(st.session_state['risk_news'][:10]):
            with st.expander(f"[{item['ê°ì„±']}] {item['ì œëª©']}"):
                st.caption(item['ê²Œì‹œì¼'])
                st.write(f"ì›ì œ: {item['ì›ì œ']}")
                st.markdown(f"[ê¸°ì‚¬ ë³´ê¸°]({item['ë§í¬']})")
                if st.button("ìš”ì•½ (ì˜ë¬¸ ê¸°ì‚¬)", key=f"risk_{i}"):
                    summary = get_article_summary(item['ë§í¬'])
                    st.success(translate_text(summary))

# --- [Tab 2] ì‚°ì§€ë³„ ë™í–¥ (Google) ---
with tab2:
    st.subheader("ì£¼ìš” ì‚°ì§€ë³„ ë™í–¥")
    country = st.selectbox("êµ­ê°€ ì„ íƒ", ["Brazil", "Vietnam", "Colombia", "Ethiopia", "Indonesia", "Kenya", "Honduras", "Guatemala", "Costa Rica", "Peru"])
    
    def get_params(c):
        if c == "Vietnam": return '"Vietnam Coffee" (Export OR Production OR Price)', ["Vietnam", "Robusta"]
        elif c == "Brazil": return '"Brazil Coffee" (Harvest OR Export OR Crop)', ["Brazil", "Arabica"]
        else: return f'"{c} Coffee" (Export OR Price)', [c]

    if st.button(f"{country} ë‰´ìŠ¤ ê²€ìƒ‰ (Google)", key="btn_origin"):
        with st.spinner('í•´ì™¸ ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„ ì¤‘...'):
            query, targets = get_params(country)
            period = '365d' if country in ["Guatemala", "Costa Rica", "Peru", "Honduras", "Kenya"] else '90d'
            st.session_state['origin_news'] = fetch_google_news(query, targets, period=period)
            
    if st.session_state['origin_news']:
        display_wordcloud(st.session_state['origin_news'])
        st.divider()
        for i, item in enumerate(st.session_state['origin_news'][:10]):
            with st.expander(f"[{item['ê°ì„±']}] {item['ì œëª©']}"):
                st.caption(item['ê²Œì‹œì¼'])
                st.write(f"ì›ì œ: {item['ì›ì œ']}")
                st.markdown(f"[ê¸°ì‚¬ ë³´ê¸°]({item['ë§í¬']})")
                if st.button("ìš”ì•½ (ì˜ë¬¸ ê¸°ì‚¬)", key=f"origin_{i}"):
                    summary = get_article_summary(item['ë§í¬'])
                    st.success(translate_text(summary))

# --- [Tab 3] êµ­ë‚´ ë‰´ìŠ¤ (Naver API) ---
with tab3:
    st.subheader("ğŸ‡°ğŸ‡· êµ­ë‚´ ì»¤í”¼ ì‹œì¥ & ì›ë‘ ë‰´ìŠ¤")
    
    # API í‚¤ í™•ì¸ ë©”ì‹œì§€
    if "ë³¸ì¸ì˜" in NAVER_CLIENT_ID:
        st.warning("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì½”ë“œ ìƒë‹¨ì„ í™•ì¸í•´ì£¼ì„¸ìš”!")
    
    korea_keyword = st.radio("ê´€ì‹¬ í‚¤ì›Œë“œ ì„ íƒ", ["ì»¤í”¼ ì›ë‘ ê°€ê²©", "ìƒë‘ ìˆ˜ì…", "ì¹´í˜ ì°½ì—… ì‹œì¥", "ìŠ¤í˜ì…œí‹° ì»¤í”¼", "ì €ê°€ ì»¤í”¼ í”„ëœì°¨ì´ì¦ˆ"], horizontal=True)
    
    if st.button("êµ­ë‚´ ë‰´ìŠ¤ ê²€ìƒ‰ (Naver API)", key="btn_korea"):
        with st.spinner(f"ë„¤ì´ë²„ì—ì„œ '{korea_keyword}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤..."):
            st.session_state['korea_news'] = fetch_naver_news_api(korea_keyword)
            
    if st.session_state['korea_news']:
        st.success(f"ê²€ìƒ‰ ê²°ê³¼ {len(st.session_state['korea_news'])}ê±´")
        for i, item in enumerate(st.session_state['korea_news']):
            with st.container():
                st.markdown(f"**{i+1}. {item['ì œëª©']}**")
                st.caption(f"ğŸ“… {item['ê²Œì‹œì¼']}")
                st.markdown(f"[ê¸°ì‚¬ ì›ë¬¸ ì½ê¸°]({item['ë§í¬']})")
                st.divider()