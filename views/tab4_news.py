# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ“ views/tab4_news.py - ê¸€ë¡œë²Œ & ë¡œì»¬ ì»¤í”¼ ë‰´ìŠ¤ ì¸ì‚¬ì´íŠ¸
================================================================================
Google RSSì™€ ë„¤ì´ë²„ APIë¥¼ í™œìš©í•œ ì»¤í”¼ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„
================================================================================
"""

import streamlit as st
import feedparser
import requests
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from textblob import TextBlob
from deep_translator import GoogleTranslator
from newspaper import Article, Config
import nltk

# ê²½ë¡œ ì„¤ì •
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import NAVER_CLIENT_ID, NAVER_CLIENT_SECRET

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)


# ===========================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ===========================================
@st.cache_resource
def get_translator():
    return GoogleTranslator(source='auto', target='ko')


def translate_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
    try:
        if not text:
            return ""
        translator = get_translator()
        return translator.translate(text[:4999])
    except:
        return text


def get_article_summary(url):
    """ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½"""
    try:
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'
        config.request_timeout = 10
        article = Article(url, config=config)
        article.download()
        article.parse()
        article.nlp()
        return article.summary if article.summary else "âš ï¸ ìš”ì•½ ì‹¤íŒ¨"
    except Exception as e:
        return f"ğŸš« ì—ëŸ¬: {str(e)}"


def analyze_sentiment(text):
    """ê°ì„± ë¶„ì„"""
    blob = TextBlob(text)
    score = blob.sentiment.polarity
    if score > 0.1:
        return "ğŸŸ¢ ê¸ì •ì "
    elif score < -0.1:
        return "ğŸ”´ ë¶€ì •ì "
    return "âšª ì¤‘ë¦½ì "


def display_wordcloud(news_list):
    """ì›Œë“œí´ë¼ìš°ë“œ í‘œì‹œ"""
    if not news_list:
        return
    text = " ".join([item.get('ì›ì œ', '') for item in news_list])
    if not text.strip():
        return
    
    try:
        wc = WordCloud(width=800, height=400, background_color='white', colormap='copper', max_words=80).generate(text)
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
        plt.close(fig)
    except:
        st.warning("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨")


# ===========================================
# Google ë‰´ìŠ¤ ìˆ˜ì§‘ (í•´ì™¸)
# ===========================================
def fetch_google_news(query, target_keywords=None, period='30d'):
    """Google RSSë¡œ í•´ì™¸ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    noise_filter = "-Starbucks -store -closing -travel -hotel"
    full_query = f"{query} {noise_filter}"
    encoded_query = full_query.replace(" ", "%20")
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}+when:{period}&hl=en-US&gl=US&ceid=US:en"
    
    feed = feedparser.parse(rss_url)
    news_list = []
    seen_titles = set()
    coffee_guard_terms = ["coffee", "bean", "arabica", "robusta", "commodity", "harvest", "crop", "farm", "export"]

    if not feed.entries:
        return []

    count = 0
    for entry in feed.entries[:100]:
        if count >= 30:
            break
        
        title_en = entry.title
        link = entry.link
        summary_text = entry.get('summary', '')
        title_signature = title_en[:30].lower()
        
        if title_signature in seen_titles:
            continue
        
        content_to_check = (title_en + " " + summary_text).lower()
        
        if target_keywords:
            has_target = any(k.lower() in content_to_check for k in target_keywords)
            has_coffee_context = any(term in content_to_check for term in coffee_guard_terms)
            if not (has_target and has_coffee_context):
                continue

        seen_titles.add(title_signature)
        sentiment = analyze_sentiment(title_en)
        korean_title = translate_text(title_en)
        
        news_list.append({
            "ì œëª©": korean_title,
            "ì›ì œ": title_en,
            "ë§í¬": link,
            "ê²Œì‹œì¼": entry.get('published', ''),
            "ê°ì„±": sentiment
        })
        count += 1
    
    return news_list


# ===========================================
# ë„¤ì´ë²„ ë‰´ìŠ¤ API (êµ­ë‚´)
# ===========================================
def fetch_naver_news_api(query):
    """ë„¤ì´ë²„ APIë¡œ êµ­ë‚´ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    if "ë„¤ì´ë²„" in NAVER_CLIENT_ID or not NAVER_CLIENT_ID:
        return [{"ì œëª©": "âš ï¸ API í‚¤ ë¯¸ì„¤ì •: config.pyì— í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "ë§í¬": "#", "ê²Œì‹œì¼": "", "ì–¸ë¡ ì‚¬": "ì‹œìŠ¤í…œ"}]

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {"query": query, "display": 10, "sort": "sim"}

    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        if response.status_code == 200:
            items = response.json().get('items', [])
            results = []
            for item in items:
                clean_title = re.sub('<.*?>', '', item['title']).replace("&quot;", "'").replace("&amp;", "&")
                link = item.get('originallink') or item.get('link')
                pub_date = item.get('pubDate', '')[:16]
                
                results.append({
                    "ì œëª©": clean_title,
                    "ë§í¬": link,
                    "ê²Œì‹œì¼": pub_date,
                    "ì–¸ë¡ ì‚¬": "ë„¤ì´ë²„ë‰´ìŠ¤"
                })
            return results
        return [{"ì œëª©": f"âš ï¸ í†µì‹  ì˜¤ë¥˜ (Code: {response.status_code})", "ë§í¬": "#", "ê²Œì‹œì¼": "", "ì–¸ë¡ ì‚¬": "ì˜¤ë¥˜"}]
    except Exception as e:
        return [{"ì œëª©": f"âš ï¸ ì—ëŸ¬: {str(e)}", "ë§í¬": "#", "ê²Œì‹œì¼": "", "ì–¸ë¡ ì‚¬": "ì˜¤ë¥˜"}]


# ===========================================
# ë©”ì¸ show() í•¨ìˆ˜
# ===========================================
def show():
    """ë‰´ìŠ¤ ì¸ì‚¬ì´íŠ¸ í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    
    st.markdown("<h1 style='text-align: center;'>ê¸€ë¡œë²Œ & ë¡œì»¬ ì»¤í”¼ ì¸ì‚¬ì´íŠ¸</h1>", unsafe_allow_html=True)
    st.divider()

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'risk_news' not in st.session_state:
        st.session_state['risk_news'] = []
    if 'origin_news' not in st.session_state:
        st.session_state['origin_news'] = []
    if 'korea_news' not in st.session_state:
        st.session_state['korea_news'] = []

    tab1, tab2, tab3 = st.tabs([" ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬", " ì‚°ì§€ë³„ ë™í–¥", " êµ­ë‚´ ì‹œì¥"])

    # ===========================================
    # Tab 1: ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬
    # ===========================================
    with tab1:
        st.subheader("ê¸€ë¡œë²Œ ê³µê¸‰ë§ & ì •ì±… ë¦¬ìŠ¤í¬")
        
        if st.button("ë¦¬ìŠ¤í¬ ë‰´ìŠ¤ ê²€ìƒ‰ (Google)", key="btn_risk"):
            with st.spinner('í•´ì™¸ ë‰´ìŠ¤ ë¶„ì„ ì¤‘...'):
                q = "Coffee Supply Chain OR EUDR Regulation OR Red Sea Logistics"
                targets = ["Coffee", "EUDR", "Red Sea", "Supply", "Logistics", "Price"]
                st.session_state['risk_news'] = fetch_google_news(q, targets, period='365d')
                
        if st.session_state['risk_news']:
            display_wordcloud(st.session_state['risk_news'])
            st.divider()
            for i, item in enumerate(st.session_state['risk_news'][:10]):
                with st.expander(f"[{item['ê°ì„±']}] {item['ì œëª©']}"):
                    st.caption(item['ê²Œì‹œì¼'])
                    st.write(f"ì›ì œ: {item['ì›ì œ']}")
                    st.markdown(f"[ê¸°ì‚¬ ë³´ê¸°]({item['ë§í¬']})")
                    if st.button("ìš”ì•½ (ì˜ë¬¸ ê¸°ì‚¬)", key=f"risk_{i}"):
                        summary = get_article_summary(item['ë§í¬'])
                        st.success(translate_text(summary))

    # ===========================================
    # Tab 2: ì‚°ì§€ë³„ ë™í–¥
    # ===========================================
    with tab2:
        st.subheader("ì£¼ìš” ì‚°ì§€ë³„ ë™í–¥")
        country = st.selectbox("êµ­ê°€ ì„ íƒ", ["Brazil", "Vietnam", "Colombia", "Ethiopia", "Indonesia", "Kenya"], key="news_country")
        
        def get_params(c):
            if c == "Vietnam":
                return '"Vietnam Coffee" (Export OR Production)', ["Vietnam", "Robusta"]
            elif c == "Brazil":
                return '"Brazil Coffee" (Harvest OR Export)', ["Brazil", "Arabica"]
            else:
                return f'"{c} Coffee" (Export OR Price)', [c]

        if st.button(f"{country} ë‰´ìŠ¤ ê²€ìƒ‰", key="btn_origin"):
            with st.spinner('ë‰´ìŠ¤ ë¶„ì„ ì¤‘...'):
                query, targets = get_params(country)
                st.session_state['origin_news'] = fetch_google_news(query, targets, period='90d')
                
        if st.session_state['origin_news']:
            display_wordcloud(st.session_state['origin_news'])
            st.divider()
            for i, item in enumerate(st.session_state['origin_news'][:10]):
                with st.expander(f"[{item['ê°ì„±']}] {item['ì œëª©']}"):
                    st.caption(item['ê²Œì‹œì¼'])
                    st.write(f"ì›ì œ: {item['ì›ì œ']}")
                    st.markdown(f"[ê¸°ì‚¬ ë³´ê¸°]({item['ë§í¬']})")
                    if st.button("ìš”ì•½", key=f"origin_{i}"):
                        summary = get_article_summary(item['ë§í¬'])
                        st.success(translate_text(summary))

    # ===========================================
    # Tab 3: êµ­ë‚´ ë‰´ìŠ¤ (ë„¤ì´ë²„ API)
    # ===========================================
    with tab3:
        st.subheader("êµ­ë‚´ ì»¤í”¼ ì‹œì¥ & ì›ë‘ ë‰´ìŠ¤")
        
        if "ë„¤ì´ë²„" in NAVER_CLIENT_ID:
            st.warning("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. config.pyë¥¼ í™•ì¸í•˜ì„¸ìš”!")
        
        korea_keyword = st.radio(
            "ê´€ì‹¬ í‚¤ì›Œë“œ ì„ íƒ", 
            ["ì»¤í”¼ ì›ë‘ ê°€ê²©", "ìƒë‘ ìˆ˜ì…", "ì¹´í˜ ì°½ì—… ì‹œì¥", "ìŠ¤í˜ì…œí‹° ì»¤í”¼", "ì €ê°€ ì»¤í”¼ í”„ëœì°¨ì´ì¦ˆ"], 
            horizontal=True,
            key="korea_keyword"
        )
        
        if st.button("êµ­ë‚´ ë‰´ìŠ¤ ê²€ìƒ‰ (Naver API)", key="btn_korea"):
            with st.spinner(f"'{korea_keyword}' ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
                st.session_state['korea_news'] = fetch_naver_news_api(korea_keyword)
                
        if st.session_state['korea_news']:
            st.success(f"ê²€ìƒ‰ ê²°ê³¼ {len(st.session_state['korea_news'])}ê±´")
            for i, item in enumerate(st.session_state['korea_news']):
                with st.container():
                    st.markdown(f"**{i+1}. {item['ì œëª©']}**")
                    st.caption(f" {item['ê²Œì‹œì¼']}")
                    st.markdown(f"[ê¸°ì‚¬ ì›ë¬¸ ì½ê¸°]({item['ë§í¬']})")
                    st.divider()


if __name__ == "__main__":
    show()
